#GPT-4 calls will be here...

# llm.py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

MODEL_NAME = "MBZUAI/LaMini-Flan-T5-783M"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Loading model {MODEL_NAME} on {DEVICE}...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)

def synthesize(question: str, evidence_snippets: list, max_new_tokens: int = 256) -> dict:
    snippet_text = "\n".join([f"[{i+1}] {s.get('text')}" for i, s in enumerate(evidence_snippets[:5])])
    prompt = (
        f"You are a precise assistant. Using only the provided document excerpts, "
        f"answer the question.\n\nQuestion: {question}\nExcerpts:\n{snippet_text}\n\n"
        f"Give a short, direct answer with any numeric values, and briefly explain why."
    )
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(DEVICE)
    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=0.0)
    answer_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return {
        "answer": answer_text.strip(),
        "quotes": None,
        "confidence": 0.7,
        "rationale": "Generated by local LaMini-Flan-T5 model based on retrieved clauses."
    }
